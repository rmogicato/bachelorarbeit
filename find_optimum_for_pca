import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

files = ["mean.csv", "std.csv"]
colors = ["blue", "red"]

for idx, file in enumerate(files):
    expl_var = []
    n = []

    for i in range(30):
        df_raw = pd.read_csv(file, header=0, index_col=0)
        # dropping all rows where there are fewer than 5 images for a person
        df_raw = df_raw.drop(df_raw[df_raw.n < i].index)
        df = df_raw.drop(columns="n")

        # scaling data?
        pca = PCA()
        pca.fit(df)
        pca_data = pca.transform(df)
        percentage_var = np.round(pca.explained_variance_ratio_ * 100, decimals=1)
        explained = percentage_var[0] + percentage_var[1]
        expl_var.append(explained)
        n.append(df.shape[0])
    print(n)

    fig, ax = plt.subplots()
    ax.plot(n, expl_var, "bo")
    ax.set_xlim(10200, 2000)
    ax.set_title(file)
    ax.set_ylabel("variance explained by first two PC")
    ax.set_xlabel("number of identities")
    plt.show()

